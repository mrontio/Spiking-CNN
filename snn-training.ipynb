{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0d3330-3069-41b4-92b1-9da97132c4ca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tonic.datasets.nmnist import NMNIST\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "root_dir = './nmnist'\n",
    "_ = NMNIST(save_to=root_dir, train=True)\n",
    "_ = NMNIST(save_to=root_dir, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "221ac35f-805a-414c-9e69-ecc184c9c339",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tonic.transforms import ToFrame\n",
    "from tonic.datasets import nmnist\n",
    "\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "device = \"cuda:0\"\n",
    "shuffle = True\n",
    "\n",
    "# Transform that accumulates events into single frame image\n",
    "to_frame = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=1)\n",
    "\n",
    "train_dataset = NMNIST(save_to=root_dir, train=True, transform=to_frame)\n",
    "test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_frame)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=shuffle)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc579952-7899-48c4-b5d6-ede02e0e1d69",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_linear = nn.Sequential(\n",
    "    # [2, 34, 34] -> [10]\n",
    "    nn.Linear(2 * 34 * 34, 10),\n",
    "    nn.ReLU(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d014191a-9f3a-4049-8d71-17317f8e3793",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_conv_linear = nn.Sequential(\n",
    "    # [2, 34, 34] -> [8, 17, 17]\n",
    "    nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(3,3), padding=(1,1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(9248, 10, bias=False),\n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff798178-43e2-4185-b587-17c6ff6ec6aa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_conv_avg_linear = nn.Sequential(\n",
    "    # [2, 34, 34] -> [8, 17, 17]\n",
    "    nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(3,3), padding=(1,1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(9248, 10, bias=False),\n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ae85f35-53da-4212-beaa-59c5efbe013e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_sinabs = nn.Sequential(\n",
    "    # [2, 34, 34] -> [8, 17, 17]\n",
    "    nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(3,3), padding=(1,1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    \n",
    "    # [8, 17, 17] -> [16, 8, 8]\n",
    "    nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3), padding=(1,1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2,2),\n",
    "    \n",
    "    # [16, 8, 8] -> [16, 4, 4]\n",
    "    nn.Conv2d(in_channels=16, out_channels=16, kernel_size = (3,3), padding=(1,1), stride=(2,2), bias=False),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    # [16 * 4 * 4] -> [10]\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 4 * 4, 10, bias=False),\n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "145b385a-7849-45cf-ac69-5e157d1ad864",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "model = model_sinabs\n",
    "\n",
    "epochs = 10\n",
    "lr = 1e-3\n",
    "\n",
    "# init the model weights\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_normal_(layer.weight.data)\n",
    "\n",
    "optimizer = SGD(params=model.parameters(), lr=lr)\n",
    "criterion = CrossEntropyLoss()\n",
    "model.to(device)\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    # train\n",
    "    train_p_bar = tqdm(train_dataloader, desc=f'Epoch {e+1}/{epochs}', leave=False, mininterval=1)\n",
    "    for data, label in train_p_bar:\n",
    "        # remove the time-step axis since we are training model\n",
    "        # move the data to accelerator\n",
    "        data = data.squeeze(dim=1).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # set progressing bar\n",
    "        #train_p_bar.set_postfix(loss=round(loss.item(), 4), mininterval=1)\n",
    "\n",
    "    # validate\n",
    "    correct_predictions = []\n",
    "    with torch.no_grad():\n",
    "        test_p_bar = tqdm(test_dataloader, desc=f'Epoch {e}, testing model.', mininterval=1)\n",
    "        for data, label in test_p_bar:\n",
    "            # remove the time-step axis since we are training model\n",
    "            # move the data to accelerator\n",
    "            data = data.squeeze(dim=1).to(dtype=torch.float, device=device)\n",
    "            label = label.to(dtype=torch.long, device=device)\n",
    "            # forward\n",
    "            output = model(data)\n",
    "            # calculate accuracy\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # compute the total correct predictions\n",
    "            correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "    \n",
    "        correct_predictions = torch.cat(correct_predictions)\n",
    "        print(f\"\\nEpoch {e} - accuracy: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")\n",
    "torch.save(model.state_dict(), \"./weights/model-weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54bbcdc-203e-4b93-9e5f-baee4996c409",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2202997/3734851863.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"./weights/model-weights.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_sinabs\n",
    "model.load_state_dict(torch.load(\"./weights/model-weights.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "456ea942-e016-421d-bd93-b08d76385aee",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=1, num_timesteps=-1)\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=1, num_timesteps=-1)\n",
       "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (7): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=1, num_timesteps=-1)\n",
       "  (8): Flatten(start_dim=1, end_dim=-1)\n",
       "  (9): Linear(in_features=256, out_features=10, bias=False)\n",
       "  (10): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=1, num_timesteps=-1)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sinabs.from_torch import from_model\n",
    "import sinabs.activation.spike_generation as spikegen\n",
    "\n",
    "snn = from_model(model=model, input_shape=(2, 34, 34), batch_size=1, spike_fn=spikegen.SingleSpike).spiking_model\n",
    "snn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2a3db6c-87c5-4635-aed1-8c214b73adc7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_time_steps = 100\n",
    "to_raster = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=n_time_steps)\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_raster)\n",
    "snn_test_dataloader = DataLoader(snn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=False)\n",
    "\n",
    "snn = snn.to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be069db8-564f-4868-8723-a69fc6f0475f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "correct_predictions = []\n",
    "with torch.no_grad():\n",
    "    test_p_bar = tqdm(snn_test_dataloader,  desc=f'Epoch {e}, testing model.', mininterval=1)\n",
    "    for data, label in test_p_bar:\n",
    "        # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "        data = data.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        # forward\n",
    "        output = snn(data)\n",
    "        # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "        output = output.reshape(batch_size, n_time_steps, -1)\n",
    "        # accumulate all time-steps output for final prediction\n",
    "        output = output.sum(dim=1)\n",
    "        # calculate accuracy\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        # compute the total correct predictions\n",
    "        correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "        # set progressing bar\n",
    "        test_p_bar.update()\n",
    "        #test_p_bar.set_description(f\"Testing SNN Model...\")\n",
    "\n",
    "    correct_predictions = torch.cat(correct_predictions)\n",
    "    print(f\"\\nAccuracy of converted SNN: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58447efb-57fd-47b2-889d-762f201951db",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "def dataloader_to_numpy(dataloader):\n",
    "    count = 0\n",
    "    for batch in dataloader:\n",
    "        for i in range(batch[0].shape[0]):\n",
    "            x = batch[0][i]\n",
    "            y = batch[1][i]\n",
    "            saved_x = x.cpu().detach().numpy()\n",
    "            np.save(f'/home/mrontio/data/nmnist-converted/{y}/{count}.npy', saved_x)\n",
    "            count += 1\n",
    "\n",
    "dataloader_to_numpy(snn_test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7a619e2-24ca-4731-8919-b9fbefcfa2e5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2, 34, 34)\n",
      "tensor([[0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sample_data, label = snn_test_dataset[0]\n",
    "print(sample_data.shape)\n",
    "sample_data = torch.tensor(sample_data).reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "logits = snn(sample_data).reshape(1, 100, -1)\n",
    "output = logits.sum(dim=1)\n",
    "pred = output.argmax(dim=1, keepdim=True)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44e1b1c9-e372-46e2-b2a9-890c08072ece",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "_ = snn.to('cpu')\n",
    "zeros = torch.zeros(sample_data.shape)\n",
    "ones = torch.ones(sample_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "016078e2-2f23-4ce7-b971-c78a220a97e6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spike_threshold: 1.0\n",
      "min_v_mem: -1.0\n"
     ]
    }
   ],
   "source": [
    "modules = list(snn.modules())[1:]\n",
    "print(f'spike_threshold: {float(modules[1].spike_threshold)}\\nmin_v_mem: {float(modules[1].min_v_mem)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86ab4e7e-674c-4809-af0c-8101e836b9cd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.3294, grad_fn=<MinBackward1>) tensor(3.9885, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "conv = modules[0]\n",
    "conv_out = conv(ones)\n",
    "iaf = modules[1]\n",
    "iaf_out = iaf(conv_out)\n",
    "\n",
    "print(torch.min(conv_out), torch.max(conv_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0897f249-32d9-42bd-ae69-c4c8744d39d2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 8, 34, 34])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = list(conv.parameters())[0]\n",
    "W.shape\n",
    "conv_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c99dc87b-edb8-461f-a8f3-39650356d4ee",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_tensor(tensor, ax, printcb=False):\n",
    "    if tensor.dim() != 2:\n",
    "        raise ValueError(\"Input tensor must be a 2D tensor.\")\n",
    "    \n",
    "    tensor_np = tensor.numpy()  # Convert to numpy array\n",
    "    ax.imshow(tensor_np, cmap='coolwarm', aspect='equal')\n",
    "\n",
    "    cax = ax.imshow(tensor_np, cmap='coolwarm', aspect='equal')\n",
    "    if printcb:\n",
    "        fig.colorbar(cax, ax=ax)  # Show color scale\n",
    "    ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71660d91-3d7b-4553-9a14-7b4d2eb4bad9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_class_name(class_string):\n",
    "    # Strip the angle brackets and split the string by dots\n",
    "    split_string = class_string.strip(\"<>\").split(\".\")\n",
    "    # Get the last element from the split string\n",
    "    class_name = split_string[-1]\n",
    "    # Remove the ending single quote if it exists\n",
    "    return class_name.strip(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab1508ea-50bb-4c23-ba2a-6c32643553a2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 30, 100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x700 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sinabs.layers import IAF,LIF\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#tensor = torch.rand((1, 2, 34, 34))\n",
    "#tensor[0,1] = tensor[0,0]\n",
    "begin = 0\n",
    "end = 3\n",
    "tensor = torch.linspace(begin, end, steps=100).repeat(30, 1)\n",
    "iaf = IAF(spike_fn = spikegen.SingleSpike, record_states=True)\n",
    "\n",
    "t = 5\n",
    "\n",
    "image = tensor.unsqueeze(0)\n",
    "image = torch.stack([image] * t, dim=1)\n",
    "\n",
    "title = f'Linspace [{begin},{end}], {get_class_name(str(iaf.spike_fn))}'\n",
    "\n",
    "# image[0, 1:3] = torch.zeros((30, 100))\n",
    "# image[0, 3] = torch.zeros((30, 100))\n",
    "# title += ', with interruptions'\n",
    "#image = sample_data[10].unsqueeze(0).cpu()\n",
    "print(image.shape)\n",
    "fig, ax = plt.subplots(image.shape[1], 2, figsize=(5,7))\n",
    "for i in range(image.shape[1]):\n",
    "    with torch.no_grad():\n",
    "        plot_tensor(image[0,i], ax[i,0])\n",
    "        plot_tensor(iaf(image)[0,i], ax[i, 1])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.suptitle(title)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72128438-19ef-40fc-86e5-d46ea79d3ca0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 30, 100])\n",
      "tensor(0.9091)\n",
      "0.9091 -> 0.0909\n",
      "0.9091 -> 0.0000\n",
      "0.9091 -> 0.9091\n",
      "0.9091 -> 0.8182\n",
      "0.9091 -> 0.7273\n"
     ]
    }
   ],
   "source": [
    "recordings = iaf.recordings['v_mem']\n",
    "x = 30\n",
    "y = 0\n",
    "print(recordings.shape)\n",
    "print(image[0, 0, y, x])\n",
    "for i in range(recordings.shape[1]):\n",
    "    print(f'{float(image[0,i,y,x]):.4f} -> {float(recordings[0,i,y,x]):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32df026e-0bca-41f4-9b81-410c55fde028",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def conv_layer(x: 'np.ndarray[np.float32]', W):\n",
    "    C_in = W.shape[1]\n",
    "    C_out = W.shape[0]\n",
    "    kernel_size = tuple((W.shape[2:4]))\n",
    "    stride = 1\n",
    "    padding = (1,1)\n",
    "    # Zero pad\n",
    "    data_padded = torch.zeros(x.shape[0],\n",
    "                              x.shape[1] + 2*padding[0],\n",
    "                              x.shape[2] + 2*padding[0])\n",
    "    data_padded[:, padding[0]:x.shape[1] + padding[0], padding[1]:x.shape[2] + padding[1]] = x.cpu()\n",
    "    output = torch.zeros(C_out, x.shape[1], x.shape[2])\n",
    "    \n",
    "    coordinate_pairs = [(a, b) for a in range(0, x.shape[1], stride) for b in range(0, x.shape[2], stride)]\n",
    "    for l in range(C_out):\n",
    "        for k in range(C_in):\n",
    "            kernel = W[l,k]\n",
    "            for i, j in coordinate_pairs:\n",
    "                i_pad = i + padding[0]\n",
    "                j_pad = j + padding[1]\n",
    "                for m_ in range(kernel_size[0]):\n",
    "                    m = m_ - 1\n",
    "                    for n_ in range(kernel_size[1]):\n",
    "                        n = n_ -1\n",
    "                        output[l, i, j] += (kernel[m_, n_]\n",
    "                                            * data_padded[k, i_pad + m, j_pad + n])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "x = torch.ones([2, 34, 34])\n",
    "# sample_data.to('cpu')\n",
    "# x = sample_data[0].to('cpu')\n",
    "conv_theirs = conv(x)\n",
    "conv_mine = conv_layer(x, W)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6902c5a-449e-482f-9e1a-39f78d4bf83b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: 9248, False: 0\n",
      "tensor(-0.5376, grad_fn=<SelectBackward0>)\n",
      "tensor(-0.5376, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "precision = 5\n",
    "mine_round = torch.round(conv_mine * 10**precision) / 10**precision\n",
    "theirs_round = torch.round(conv_theirs * 10**precision) / 10**precision\n",
    "num_true = torch.sum(mine_round == theirs_round).item()\n",
    "num_false = (mine_round==theirs_round).numel() - num_true\n",
    "print(f'True: {num_true}, False: {num_false}')\n",
    "false_indexes = torch.nonzero((conv_mine == conv_theirs) == False, as_tuple=False)\n",
    "print(conv_theirs[0,0,0])\n",
    "print(conv_mine[0,0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b54109d7-ab59-4bd7-a323-77a4fb4573af",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "weights_np = W.detach().numpy() \n",
    "theirs_np = conv_theirs.detach().numpy()\n",
    "mine_np = conv_mine.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1f1c0c8-e136-42dd-8cdc-fe236c2a4f5c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 34, 34])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_mine.reshape(1, 1, 8, 34, 34).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd00d71a-e630-47f4-a54c-3a2ea9837c86",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class if_layer:\n",
    "    def __init__(self, input_shape):\n",
    "        self.membrane = torch.zeros(input_shape)\n",
    "        self.coordinate_pairs = [(a, b) for a in range(input_shape[1]) for b in range(input_shape[2])]\n",
    "        self.v_th = 1.0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.membrane = torch.zeros(input_shape)\n",
    "    def __call__(self, x: 'np.ndarray[np.float32]'):\n",
    "        '''Basic implementation of IF layer.\n",
    "        Assumptions:\n",
    "        spike_threshold = 1.0,\n",
    "        spike_fn = SingleSpike\n",
    "        No batch, No timing: the FPGA simply runs everything on FIFO basis\n",
    "        Thererofe, this has to be a class.\n",
    "        x dimensions: [c, y, x]\n",
    "        '''\n",
    "        output = torch.zeros(self.membrane.shape)\n",
    "        for c in range(x.shape[0]):\n",
    "            for (i, j) in self.coordinate_pairs:\n",
    "                self.membrane[c, i, j] +=  x[c, i, j]\n",
    "\n",
    "                if self.membrane[c, i, j] > self.v_th:\n",
    "                    output[c,i,j] = 1;\n",
    "                    self.membrane[c, i, j] -= 1\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42b49e24-f411-498e-b1fc-8cc12dfc9bb6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class if1_layer:\n",
    "    def __init__(self, input_shape):\n",
    "        self.elems = torch.tensor(input_shape).prod().item()\n",
    "        self.membrane = torch.zeros([self.elems])\n",
    "        self.coordinate_pairs = [(a, b) for a in range(input_shape[1]) for b in range(input_shape[2])]\n",
    "        self.v_th = 1.0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.membrane = torch.zeros(input_shape)\n",
    "    def __call__(self, x: 'np.ndarray[np.float32]'):\n",
    "        '''Basic implementation of IF layer.\n",
    "        Assumptions:\n",
    "        spike_threshold = 1.0,\n",
    "        spike_fn = SingleSpike\n",
    "        No batch, No timing: the FPGA simply runs everything on FIFO basis\n",
    "        Thererofe, this has to be a class.\n",
    "        x dimensions: [c, y, x]\n",
    "        '''\n",
    "        input_shape = x.shape\n",
    "        x = x.reshape((self.elems,))\n",
    "        output = torch.zeros(self.elems)\n",
    "        for e in range(self.elems):\n",
    "            self.membrane[e] +=  x[e]\n",
    "            if self.membrane[e] > self.v_th:\n",
    "                output[e] = 1;\n",
    "                self.membrane[e] -= 1\n",
    "\n",
    "        output = output.reshape(input_shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3128d58-e124-4c31-b533-1e0e30b97466",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "my_if = if1_layer(conv_mine.shape)\n",
    "iaf = IAF(spike_fn = spikegen.SingleSpike)\n",
    "\n",
    "ifs_mine = [my_if(conv_mine) for _ in range(10)]\n",
    "ifs_theirs = [iaf(conv_mine.reshape(1,1,8,34,34)) for _ in range(10)]\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'{torch.nonzero(ifs_mine[i]).shape[0]} -> ', end='')\n",
    "print()\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'{torch.nonzero(ifs_theirs[i]).shape[0]} -> ', end='')\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e5cc6fe-bbbf-46f8-94e5-934135af5d64",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ifs_theirs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m if_torch_np \u001b[38;5;241m=\u001b[39m \u001b[43mifs_theirs\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./tensors/if-torch.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, if_torch_np)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ifs_theirs' is not defined"
     ]
    }
   ],
   "source": [
    "if_torch_np = ifs_theirs[0].detach().numpy()\n",
    "np.save('./tensors/if-torch.npy', if_torch_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5646d4b-f21d-4717-a55e-d2ec08128e2b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lel = modules[1](modules[0](torch.ones([2, 34, 34]))\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9755aa0-28fd-4865-adb0-8f06de2fd8dd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def avg_pool2d(x):\n",
    "    kernel = (2,2)\n",
    "    stride = 2\n",
    "    padding = 0\n",
    "\n",
    "    # Create output buffer\n",
    "    H_out = math.floor((x.shape[1] + (2 * padding) - kernel[0]) / stride) + 1\n",
    "    W_out = math.floor((x.shape[2] + (2 * padding) - kernel[0]) / stride) + 1\n",
    "    output = torch.zeros((x.shape[0], H_out, W_out))\n",
    "\n",
    "    # Zero pad\n",
    "    data_padded = torch.zeros(x.shape[0],\n",
    "                              x.shape[1] + 2*padding,\n",
    "                              x.shape[2] + 2*padding)\n",
    "    data_padded[:, padding:x.shape[1] + padding, padding:x.shape[2] + padding] = x.cpu()\n",
    "\n",
    "    coordinate_pairs = [(a, b) for a in range(H_out) for b in range(W_out)]\n",
    "    coordinate_pairs.sort()\n",
    "    kernel_pairs = [(a, b) for a in range(kernel[0]) for b in range(kernel[1])]\n",
    "    kernel_pairs.sort()\n",
    "    for c in range(output.shape[0]):\n",
    "        for (h, w) in coordinate_pairs:\n",
    "            # Apply kernel\n",
    "            kernel_sum = 0\n",
    "            for (m, n) in kernel_pairs:\n",
    "                kernel_sum += data_padded[c, stride * h + m, stride * w + n]\n",
    "            output[c, h, w] = 1 / (kernel[0] * kernel[1]) * kernel_sum\n",
    "\n",
    "    return output                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4030c805-33ad-4ab1-913c-a98c6069a767",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ifs_mine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mifs_mine\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m pool \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mAvgPool2d(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      4\u001b[0m pool_mine \u001b[38;5;241m=\u001b[39m avg_pool2d(x)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ifs_mine' is not defined"
     ]
    }
   ],
   "source": [
    "x = ifs_mine[0]\n",
    "pool = nn.AvgPool2d(2, 2)\n",
    "\n",
    "pool_mine = avg_pool2d(x)\n",
    "pool_torch = pool(x.unsqueeze(0))\n",
    "\n",
    "precision = 5\n",
    "mine_round = torch.round(pool_mine * 10**precision) / 10**precision\n",
    "torch_round = torch.round(pool_torch * 10**precision) / 10**precision\n",
    "num_true = torch.sum(mine_round == torch_round).item()\n",
    "num_false = (mine_round==torch_round).numel() - num_true\n",
    "print(f'True: {num_true}, False: {num_false}')\n",
    "#false_indexes = torch.nonzero((pool_mine == pool_torch) == False, as_tuple=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "585351be-db5a-4c2e-adca-6eabe7a0384f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pool_torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pool_torch_np \u001b[38;5;241m=\u001b[39m \u001b[43mpool_torch\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      2\u001b[0m pool_mine_np \u001b[38;5;241m=\u001b[39m pool_mine\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./tensors/pool-torch.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, pool_torch_np)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pool_torch' is not defined"
     ]
    }
   ],
   "source": [
    "pool_torch_np = pool_torch.detach().numpy()\n",
    "pool_mine_np = pool_mine.detach().numpy()\n",
    "\n",
    "np.save('./tensors/pool-torch.npy', pool_torch_np)\n",
    "np.save('./tensors/pool-mine.npy', pool_mine_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9a65136-851e-4874-921d-ef7b959427c9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a9d5ad3-29f5-4f02-9eab-b75013426375",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2, 34, 34)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sample_data.cpu()\n",
    "saved_x = x.detach().numpy()\n",
    "np.save('./tensors/sample-data.npy', saved_x)\n",
    "saved_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fcbd6ef-4f13-41b6-aa60-eab311ee1e6c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snn = from_model(model=model, input_shape=(2, 34, 34), batch_size=1, spike_fn=spikegen.SingleSpike).spiking_model.to('cpu')\n",
    "with torch.no_grad():\n",
    "    saved_output = snn(x.squeeze(0))\n",
    "\n",
    "np.save('./tensors/output.npy', saved_output)\n",
    "saved_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0bd8014a-d5f5-4ee7-af91-5eee35ab59d3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_module_name(input_string):\n",
    "    input_string = str(input_string)\n",
    "    # Pattern to match the function name and contents within first\n",
    "    pattern = r\"^([a-zA-Z_][a-zA-Z0-9_]*)\\(.*\"\n",
    "    \n",
    "    match = re.match(pattern, input_string)\n",
    "    if match:\n",
    "        return match.group(1)  # Return the function name\n",
    "    else:\n",
    "        return None  # If there's no match"
   ]
  },
  {
   "cell_type": "raw",
   "id": "128258d9-72d3-4d8f-ae1f-91e7214f1048",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import re\n",
    "snn = from_model(model=model, input_shape=(2, 34, 34), batch_size=1, spike_fn=spikegen.SingleSpike).spiking_model.to('cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        outs = list()\n",
    "        data = torch.tensor(np.load(f'tensors/data{i}.npy')).unsqueeze(0)\n",
    "        print(data.shape)\n",
    "        for (i, m) in enumerate(list(snn.modules())[1:]):\n",
    "            mod_name = get_module_name(m)\n",
    "            if i == stop:\n",
    "                print(f'Stopping at {i} - {mod_name}')\n",
    "                break\n",
    "            # print(f'Doing {i} - {mod_name}')\n",
    "\n",
    "            # if (mod_name == 'Conv2d') or mod_name == 'Linear':\n",
    "            #     save_name = f'./weights/{i}-{mod_name}.npy'\n",
    "            #     np.save(save_name, list(m.parameters())[0])\n",
    "            #     print(f'\\tSaved weights {save_name} : Data size {data.shape}')\n",
    "\n",
    "            # if mod_name == 'Linear':\n",
    "            #     lin = m\n",
    "\n",
    "            # if mod_name == 'IAFSqueeze':\n",
    "            #     m.record_states = True\n",
    "\n",
    "            if mod_name == 'Flatten':\n",
    "                m = nn.Flatten(start_dim=0)\n",
    "\n",
    "            data = m(data)\n",
    "            outs.append(data)\n",
    "            # print(f'outs[{len(outs)-1}] = {mod_name} :  {data.shape}')\n",
    "            # if mod_name == 'IAFSqueeze':\n",
    "            #     save_name = f'./tensors/recording-{i}-{mod_name}.npy'\n",
    "            #     np.save(save_name, m.recordings['v_mem'].squeeze())\n",
    "            #     print(f'\\tSaved recordings {save_name} : {data.shape}')\n",
    "            name = mod_name\n",
    "\n",
    "save_name = f'./tensors/output-{i-1}-{name}.npy'\n",
    "np.save(save_name, data)\n",
    "print(f'Saved {save_name} : {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b50a6427-cf04-40fc-a744-85210b63d611",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== BEGIN ITERATION 0 ======\n",
      "IAF[1] Unique non-zero recordings: 72\n",
      "IAF[4] Unique non-zero recordings: 0\n",
      "IAF[7] Unique non-zero recordings: 0\n",
      "IAF[10] Unique non-zero recordings: 0\n",
      "\n",
      "====== BEGIN ITERATION 1 ======\n",
      "IAF[1] Unique non-zero recordings: 72\n",
      "IAF[4] Unique non-zero recordings: 316\n",
      "IAF[7] Unique non-zero recordings: 0\n",
      "IAF[10] Unique non-zero recordings: 0\n",
      "\n",
      "====== BEGIN ITERATION 2 ======\n",
      "IAF[1] Unique non-zero recordings: 72\n",
      "IAF[4] Unique non-zero recordings: 304\n",
      "Failed at Conv2d\n",
      "\n",
      "Stopped: count 28\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "snn = from_model(model=model, input_shape=(2, 34, 34), batch_size=1, spike_fn=spikegen.SingleSpike).spiking_model.to('cpu')\n",
    "mods = list(snn.modules())[1:]\n",
    "outs = list()\n",
    "mems = list()\n",
    "\n",
    "data = torch.tensor(np.load(f\"tensors/datafile.npy\"))\n",
    "\n",
    "with torch.no_grad():\n",
    "    end_of_data = False\n",
    "    count = 0\n",
    "    while not end_of_data:\n",
    "        if not os.path.isfile(f\"tensors/x{count}.npy\"):\n",
    "            end_of_data = True\n",
    "            break\n",
    "\n",
    "        mod = mods[count % len(mods)]\n",
    "        mod_name = get_module_name(mod)\n",
    "\n",
    "        if count % len(mods) == 0:\n",
    "            print(f'\\n====== BEGIN ITERATION {count // len(mods)} ======')\n",
    "            batch = count // len(mods)\n",
    "            x = data[0].unsqueeze(0)\n",
    "\n",
    "        if (mod_name == 'IAFSqueeze'):\n",
    "            mod.record_states = True\n",
    "\n",
    "        # Get the data\n",
    "        x_hat = torch.tensor(np.load(f\"tensors/x{count}.npy\"))\n",
    "        x = mod(x)\n",
    "\n",
    "        correctness = torch.unique(torch.isclose(x, x_hat, atol=1e-5))\n",
    "        if len(correctness) != 1:\n",
    "            print(f'Failed at {mod_name}')\n",
    "            break\n",
    "        \n",
    "        if (mod_name == 'IAFSqueeze'):\n",
    "            recordings = mod.recordings['v_mem']\n",
    "            recordings_hat = torch.tensor(np.load(f\"tensors/membrane{count}.npy\")).reshape(recordings.shape)\n",
    "\n",
    "            mem_correct = torch.isclose(recordings_hat, recordings, atol=1e-5)\n",
    "            print(f'IAF[{count % len(mods)}] Unique non-zero recordings: {len(torch.unique(recordings))-1}')\n",
    "            if torch.unique(mem_correct) != 1:\n",
    "                print(f'Failed at {mod_name} - Membrane mismatch')\n",
    "                wrong_idx = torch.where(mem_correct == False)\n",
    "                wrong_i = 1\n",
    "                ex_pos = (int(wrong_idx[0][wrong_i]), int(wrong_idx[1][wrong_i]),\n",
    "                          int(wrong_idx[2][wrong_i]), int(wrong_idx[3][wrong_i]),\n",
    "                          int(wrong_idx[4][wrong_i]))\n",
    "                print(f'Example: {ex_pos}: {recordings[ex_pos]} != {recordings_hat[ex_pos[2:]]}')\n",
    "\n",
    "        if (mod_name == 'Conv2D'):\n",
    "            \n",
    "\n",
    "        \n",
    "        count += 1\n",
    "    print(f'\\nStopped: count {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bbab45b-1dc7-4e4d-864c-3fe6873ca0ce",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lnum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m py \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mouts\u001b[49m[lnum\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m me \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./tensors/layer\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlnum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, me: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mme\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outs' is not defined"
     ]
    }
   ],
   "source": [
    "lnum = 1\n",
    "py = np.array(outs[lnum-1]).squeeze(0)\n",
    "me = np.load(f'./tensors/layer{lnum}.npy')\n",
    "print(f'py: {py.shape}, me: {me.shape}')\n",
    "print(np.unique((py == me)))\n",
    "print((py - me).mean())\n",
    "idx = np.where(py != me)\n",
    "print(len(idx[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fc78a87-ae29-40fb-90c0-962deb581f72",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m567\u001b[39m\n\u001b[0;32m----> 2\u001b[0m yes \u001b[38;5;241m=\u001b[39m (\u001b[43midx\u001b[49m[\u001b[38;5;241m0\u001b[39m][i], idx[\u001b[38;5;241m1\u001b[39m][i], idx[\u001b[38;5;241m2\u001b[39m][i])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(yes)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(py[yes])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'idx' is not defined"
     ]
    }
   ],
   "source": [
    "i = 567\n",
    "yes = (idx[0][i], idx[1][i], idx[2][i])\n",
    "print(yes)\n",
    "print(py[yes])\n",
    "print(me[yes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d143aaac-062b-477a-b2b1-b0fd519e7cef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tensors/layer0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msinabs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m snn \u001b[38;5;241m=\u001b[39m from_model(model\u001b[38;5;241m=\u001b[39mmodel, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m34\u001b[39m, \u001b[38;5;241m34\u001b[39m), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, spike_fn\u001b[38;5;241m=\u001b[39mspikegen\u001b[38;5;241m.\u001b[39mSingleSpike)\u001b[38;5;241m.\u001b[39mspiking_model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./tensors/layer0.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#iaf = list(snn.modules())[2]\u001b[39;00m\n",
      "File \u001b[0;32m~/ml-venv/lib/python3.12/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tensors/layer0.npy'"
     ]
    }
   ],
   "source": [
    "import sinabs\n",
    "snn = from_model(model=model, input_shape=(2, 34, 34), batch_size=1, spike_fn=spikegen.SingleSpike).spiking_model.to('cpu')\n",
    "data = torch.tensor(np.load(\"./tensors/layer0.npy\"))\n",
    "x = data.clone()\n",
    "\n",
    "#iaf = list(snn.modules())[2]\n",
    "iaf = sinabs.layers.IAF(spike_fn=spikegen.SingleSpike)\n",
    "#iaf = if1_layer(x.shape)\n",
    "iaf.record_states = True\n",
    "l = 5\n",
    "\n",
    "y_hat = torch.tensor(np.load(f\"./tensors/layer{l}.npy\"))\n",
    "y = 0\n",
    "loc = (0,0,0)\n",
    "mem = 0\n",
    "for i in range(l):\n",
    "    x = data.clone()\n",
    "    y = iaf(x)\n",
    "    print(f\"{mem:.6f} + {x[loc]:.6f} = {iaf.recordings['v_mem'][loc]:.6f}, fired? {y[loc]}\")\n",
    "    #print(f\"{mem:.6f} + {x[loc]:.6f} = {iaf.membrane[0]:.6f}, fired? {y[loc]}\")\n",
    "    mem = iaf.recordings['v_mem'][loc].clone()\n",
    "    # if i == 1:\n",
    "    #     x = torch.zeros((2,34,34))\n",
    "\n",
    "print(y.sum(), y_hat.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98b437b3-a238-44ae-a40b-49ff47c4c5f4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),\n",
       " array([], dtype=int64),\n",
       " array([], dtype=int64),\n",
       " array([], dtype=int64))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isclose(x, 0.395, atol=1e-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b450f084-552b-49a3-a51f-e6f4a91e763c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tensors/layer0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./tensors/layer0.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39misclose(x, \u001b[38;5;241m0.6512748599052429\u001b[39m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)))\n\u001b[1;32m      3\u001b[0m x[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/ml-venv/lib/python3.12/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tensors/layer0.npy'"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(np.load(\"./tensors/layer0.npy\"))\n",
    "print(np.where(np.isclose(x, 0.6512748599052429, atol=1e-2)))\n",
    "x[1,1,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "name": "snn-training.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
