{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2e0d3330-3069-41b4-92b1-9da97132c4ca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tonic.datasets.nmnist import NMNIST\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "root_dir = './nmnist'\n",
    "_ = NMNIST(save_to=root_dir, train=True)\n",
    "_ = NMNIST(save_to=root_dir, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "221ac35f-805a-414c-9e69-ecc184c9c339",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tonic.transforms import ToFrame\n",
    "from tonic.datasets import nmnist\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 4\n",
    "device = \"cuda:0\"\n",
    "shuffle = False\n",
    "\n",
    "# Transform that accumulates events into single frame image\n",
    "to_frame = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=1)\n",
    "\n",
    "train_dataset = NMNIST(save_to=root_dir, train=True, transform=to_frame)\n",
    "test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_frame)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=shuffle)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4cb5afc2-1451-4094-86d5-323d5a7f420f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 34, 34])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "for d, _ in test_dataloader:\n",
    "    sample_data = d\n",
    "    break\n",
    "\n",
    "frames = sample_data.squeeze()\n",
    "print(frames.shape)\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].imshow(frames[0])\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(frames[1])\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cc579952-7899-48c4-b5d6-ede02e0e1d69",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_linear = nn.Sequential(\n",
    "    # [2, 34, 34] -> [10]\n",
    "    nn.Linear(2 * 34 * 34, 10),\n",
    "    nn.ReLU(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d014191a-9f3a-4049-8d71-17317f8e3793",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_conv_linear = nn.Sequential(\n",
    "    # [2, 34, 34] -> [8, 17, 17]\n",
    "    nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(3,3), padding=(1,1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(9248, 10, bias=False),\n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ff798178-43e2-4185-b587-17c6ff6ec6aa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_conv_avg_linear = nn.Sequential(\n",
    "    # [2, 34, 34] -> [8, 17, 17]\n",
    "    nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(3,3), padding=(1,1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(9248, 10, bias=False),\n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9ae85f35-53da-4212-beaa-59c5efbe013e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_sinabs = nn.Sequential(\n",
    "    # [2, 34, 34] -> [8, 17, 17]\n",
    "    nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(3,3), padding=(1,1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    \n",
    "    # [8, 17, 17] -> [16, 8, 8]\n",
    "    nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3), padding=(1,1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2,2),\n",
    "    \n",
    "    # [16, 8, 8] -> [16, 4, 4]\n",
    "    nn.Conv2d(in_channels=16, out_channels=16, kernel_size = (3,3), padding=(1,1), stride=(2,2), bias=False),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    # [16 * 4 * 4] -> [10]\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 4 * 4, 10, bias=False),\n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "50312ae0-4400-489e-a2b7-47fe220ac4ee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2422283/4252821774.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_weights = torch.load(\"./weights/bleh.pth\")\n"
     ]
    }
   ],
   "source": [
    "mob = model_sinabs\n",
    "mob = nn.Sequential(\n",
    "    nn.Linear(784, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10))\n",
    "torch.save(mob.state_dict(), \"./weights/bleh.pth\")\n",
    "model_weights = torch.load(\"./weights/bleh.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "066ee6f1-fa9a-45bb-b95c-6146f9808fe8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 3, 3])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = model_weights['0.weight']\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88aa3fee-f0a8-449d-bf74-e5119381ecf9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "model = model_sinabs\n",
    "\n",
    "epochs = 3\n",
    "lr = 1e-3\n",
    "\n",
    "# init the model weights\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_normal_(layer.weight.data)\n",
    "\n",
    "optimizer = SGD(params=model.parameters(), lr=lr)\n",
    "criterion = CrossEntropyLoss()\n",
    "model.to(device)\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    # train\n",
    "    train_p_bar = tqdm(train_dataloader, desc=f'Epoch {e+1}/{epochs}', leave=False, mininterval=1)\n",
    "    for data, label in train_p_bar:\n",
    "        # remove the time-step axis since we are training model\n",
    "        # move the data to accelerator\n",
    "        data = data.squeeze(dim=1).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # set progressing bar\n",
    "        #train_p_bar.set_postfix(loss=round(loss.item(), 4), mininterval=1)\n",
    "\n",
    "    # validate\n",
    "    correct_predictions = []\n",
    "    with torch.no_grad():\n",
    "        test_p_bar = tqdm(test_dataloader, desc=f'Epoch {e}, testing model.', mininterval=1)\n",
    "        for data, label in test_p_bar:\n",
    "            # remove the time-step axis since we are training model\n",
    "            # move the data to accelerator\n",
    "            data = data.squeeze(dim=1).to(dtype=torch.float, device=device)\n",
    "            label = label.to(dtype=torch.long, device=device)\n",
    "            # forward\n",
    "            output = model(data)\n",
    "            # calculate accuracy\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # compute the total correct predictions\n",
    "            correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "    \n",
    "        correct_predictions = torch.cat(correct_predictions)\n",
    "        print(f\"\\nEpoch {e} - accuracy: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")\n",
    "torch.save(model.state_dict(), \"./weights/model-weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c54bbcdc-203e-4b93-9e5f-baee4996c409",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2422283/3213821862.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_weights = torch.load(\"./weights/model-weights.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_sinabs\n",
    "model_weights = torch.load(\"./weights/model-weights.pth\")\n",
    "model.load_state_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "456ea942-e016-421d-bd93-b08d76385aee",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=1, num_timesteps=-1)\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=1, num_timesteps=-1)\n",
       "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (7): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=1, num_timesteps=-1)\n",
       "  (8): Flatten(start_dim=1, end_dim=-1)\n",
       "  (9): Linear(in_features=256, out_features=10, bias=False)\n",
       "  (10): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=1, num_timesteps=-1)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sinabs.from_torch import from_model\n",
    "import sinabs.activation.spike_generation as spikegen\n",
    "\n",
    "snn = from_model(model=model, input_shape=(2, 34, 34), batch_size=1, spike_fn=spikegen.SingleSpike).spiking_model\n",
    "snn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c2a3db6c-87c5-4635-aed1-8c214b73adc7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_time_steps = 100\n",
    "to_raster = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=n_time_steps)\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_raster)\n",
    "snn_test_dataloader = DataLoader(snn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=False)\n",
    "\n",
    "snn = snn.to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d959c184-6167-40be-9382-27485e09e5a3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "correct_predictions = []\n",
    "with torch.no_grad():\n",
    "    test_p_bar = tqdm(snn_test_dataloader,  desc=f'Testing model.', mininterval=1)\n",
    "    for data, label in test_p_bar:\n",
    "        # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "        data = data.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        # forward\n",
    "        output = snn(data)\n",
    "        # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "        output = output.reshape(batch_size, n_time_steps, -1)\n",
    "        # accumulate all time-steps output for final prediction\n",
    "        output = output.sum(dim=1)\n",
    "        # calculate accuracy\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        # compute the total correct predictions\n",
    "        correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "        # set progressing bar\n",
    "        test_p_bar.update()\n",
    "        #test_p_bar.set_description(f\"Testing SNN Model...\")\n",
    "\n",
    "    correct_predictions = torch.cat(correct_predictions)\n",
    "    print(f\"\\nAccuracy of converted SNN: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb6bd193-5509-44a0-b4da-de13a3b6a5fa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def dataloader_to_numpy(dataloader):\n",
    "    count = 0\n",
    "    for batch in dataloader:\n",
    "        for i in range(batch[0].shape[0]):\n",
    "            x = batch[0][i]\n",
    "            y = batch[1][i]\n",
    "            saved_x = x.to(device=\"cpu\", dtype=torch.float).detach().numpy()\n",
    "            np.save(f'/home/mrontio/data/nmnist-converted/{y}/{count}.npy', saved_x)\n",
    "            count += 1\n",
    "            \n",
    "\n",
    "dataloader_to_numpy(snn_test_dataloader)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bc08bd4-14f5-4b0e-a749-19fa92c39c91",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def dataloader_to_numpy(dataloader):\n",
    "    count = 0\n",
    "    for pred_class in range(10):\n",
    "        save_dict = {}\n",
    "        save_dict['count_start'] = count\n",
    "        for batch in dataloader:\n",
    "            for i in range(batch[0].shape[0]):\n",
    "                x = batch[0][i]\n",
    "                y = batch[1][i]\n",
    "                if y == pred_class:\n",
    "                    save_dict[str(count)] = x.to(device=\"cpu\").detach().numpy()\n",
    "                    count += 1\n",
    "        save_dict['count_end'] = count\n",
    "        \n",
    "        np.savez_compressed(f'/home/mrontio/data/nmnist-converted-npz/{pred_class}.npz', **save_dict)\n",
    "        del save_dict\n",
    "dataloader_to_numpy(snn_test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a7a619e2-24ca-4731-8919-b9fbefcfa2e5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2, 34, 34) 1\n",
      "tensor([[1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sample_data, label = snn_test_dataset[1000]\n",
    "print(sample_data.shape, label)\n",
    "sample_data.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44e1b1c9-e372-46e2-b2a9-890c08072ece",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "_ = snn.to('cpu')\n",
    "zeros = torch.zeros(sample_data.shape)\n",
    "ones = torch.ones(sample_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "016078e2-2f23-4ce7-b971-c78a220a97e6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spike_threshold: 1.0\n",
      "min_v_mem: -1.0\n"
     ]
    }
   ],
   "source": [
    "modules = list(snn.modules())[1:]\n",
    "print(f'spike_threshold: {float(modules[1].spike_threshold)}\\nmin_v_mem: {float(modules[1].min_v_mem)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86ab4e7e-674c-4809-af0c-8101e836b9cd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.1638, grad_fn=<MinBackward1>) tensor(2.4432, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "conv = modules[0]\n",
    "conv_out = conv(ones)\n",
    "iaf = modules[1]\n",
    "iaf_out = iaf(conv_out)\n",
    "\n",
    "print(torch.min(conv_out), torch.max(conv_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0897f249-32d9-42bd-ae69-c4c8744d39d2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 8, 34, 34])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = list(conv.parameters())[0]\n",
    "W.shape\n",
    "conv_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c99dc87b-edb8-461f-a8f3-39650356d4ee",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_tensor(tensor, ax, printcb=False):\n",
    "    if tensor.dim() != 2:\n",
    "        raise ValueError(\"Input tensor must be a 2D tensor.\")\n",
    "    \n",
    "    tensor_np = tensor.numpy()  # Convert to numpy array\n",
    "    ax.imshow(tensor_np, cmap='coolwarm', aspect='equal')\n",
    "\n",
    "    cax = ax.imshow(tensor_np, cmap='coolwarm', aspect='equal')\n",
    "    if printcb:\n",
    "        fig.colorbar(cax, ax=ax)  # Show color scale\n",
    "    ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71660d91-3d7b-4553-9a14-7b4d2eb4bad9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_class_name(class_string):\n",
    "    # Strip the angle brackets and split the string by dots\n",
    "    split_string = class_string.strip(\"<>\").split(\".\")\n",
    "    # Get the last element from the split string\n",
    "    class_name = split_string[-1]\n",
    "    # Remove the ending single quote if it exists\n",
    "    return class_name.strip(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab1508ea-50bb-4c23-ba2a-6c32643553a2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 30, 100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x700 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sinabs.layers import IAF,LIF\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#tensor = torch.rand((1, 2, 34, 34))\n",
    "#tensor[0,1] = tensor[0,0]\n",
    "begin = 0\n",
    "end = 3\n",
    "tensor = torch.linspace(begin, end, steps=100).repeat(30, 1)\n",
    "iaf = IAF(spike_fn = spikegen.SingleSpike, record_states=True)\n",
    "\n",
    "t = 5\n",
    "\n",
    "image = tensor.unsqueeze(0)\n",
    "image = torch.stack([image] * t, dim=1)\n",
    "\n",
    "title = f'Linspace [{begin},{end}], {get_class_name(str(iaf.spike_fn))}'\n",
    "\n",
    "# image[0, 1:3] = torch.zeros((30, 100))\n",
    "# image[0, 3] = torch.zeros((30, 100))\n",
    "# title += ', with interruptions'\n",
    "#image = sample_data[10].unsqueeze(0).cpu()\n",
    "print(image.shape)\n",
    "fig, ax = plt.subplots(image.shape[1], 2, figsize=(5,7))\n",
    "for i in range(image.shape[1]):\n",
    "    with torch.no_grad():\n",
    "        plot_tensor(image[0,i], ax[i,0])\n",
    "        plot_tensor(iaf(image)[0,i], ax[i, 1])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.suptitle(title)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72128438-19ef-40fc-86e5-d46ea79d3ca0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 30, 100])\n",
      "tensor(0.9091)\n",
      "0.9091 -> 0.0909\n",
      "0.9091 -> 0.0000\n",
      "0.9091 -> 0.9091\n",
      "0.9091 -> 0.8182\n",
      "0.9091 -> 0.7273\n"
     ]
    }
   ],
   "source": [
    "recordings = iaf.recordings['v_mem']\n",
    "x = 30\n",
    "y = 0\n",
    "print(recordings.shape)\n",
    "print(image[0, 0, y, x])\n",
    "for i in range(recordings.shape[1]):\n",
    "    print(f'{float(image[0,i,y,x]):.4f} -> {float(recordings[0,i,y,x]):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32df026e-0bca-41f4-9b81-410c55fde028",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def conv_layer(x: 'np.ndarray[np.float32]', W):\n",
    "    C_in = W.shape[1]\n",
    "    C_out = W.shape[0]\n",
    "    kernel_size = tuple((W.shape[2:4]))\n",
    "    timesteps = x.shape[0]\n",
    "    stride = 1\n",
    "    padding = (1,1)\n",
    "    # Zero pad\n",
    "    data_padded = torch.zeros(x.shape[0],\n",
    "                              x.shape[1] + 2*padding[0],\n",
    "                              x.shape[2] + 2*padding[0])\n",
    "    data_padded[:, padding[0]:x.shape[1] + padding[0], padding[1]:x.shape[2] + padding[1]] = x.cpu()\n",
    "    output = torch.zeros(C_out, x.shape[1], x.shape[2])\n",
    "    \n",
    "    coordinate_pairs = [(a, b) for a in range(0, x.shape[1], stride) for b in range(0, x.shape[2], stride)]\n",
    "    for l in range(C_out):\n",
    "        for k in range(C_in):\n",
    "            kernel = W[l,k]\n",
    "            for i, j in coordinate_pairs:\n",
    "                i_pad = i + padding[0]\n",
    "                j_pad = j + padding[1]\n",
    "                for m_ in range(kernel_size[0]):\n",
    "                    m = m_ - 1\n",
    "                    for n_ in range(kernel_size[1]):\n",
    "                        n = n_ -1\n",
    "                        output[l, i, j] += (kernel[m_, n_]\n",
    "                                            * data_padded[k, i_pad + m, j_pad + n])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "x = torch.ones([2, 34, 34])\n",
    "# sample_data.to('cpu')\n",
    "# x = sample_data[0].to('cpu')\n",
    "conv_theirs = conv(x)\n",
    "conv_mine = conv_layer(x, W)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6902c5a-449e-482f-9e1a-39f78d4bf83b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: 9248, False: 0\n",
      "tensor(-0.2468, grad_fn=<SelectBackward0>)\n",
      "tensor(-0.2468, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "precision = 5\n",
    "mine_round = torch.round(conv_mine * 10**precision) / 10**precision\n",
    "theirs_round = torch.round(conv_theirs * 10**precision) / 10**precision\n",
    "num_true = torch.sum(mine_round == theirs_round).item()\n",
    "num_false = (mine_round==theirs_round).numel() - num_true\n",
    "print(f'True: {num_true}, False: {num_false}')\n",
    "false_indexes = torch.nonzero((conv_mine == conv_theirs) == False, as_tuple=False)\n",
    "print(conv_theirs[0,0,0])\n",
    "print(conv_mine[0,0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b54109d7-ab59-4bd7-a323-77a4fb4573af",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "weights_np = W.detach().numpy() \n",
    "theirs_np = conv_theirs.detach().numpy()\n",
    "mine_np = conv_mine.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1f1c0c8-e136-42dd-8cdc-fe236c2a4f5c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 34, 34])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_mine.reshape(1, 1, 8, 34, 34).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd00d71a-e630-47f4-a54c-3a2ea9837c86",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class if_layer:\n",
    "    def __init__(self, input_shape):\n",
    "        self.membrane = torch.zeros(input_shape)\n",
    "        self.coordinate_pairs = [(a, b) for a in range(input_shape[1]) for b in range(input_shape[2])]\n",
    "        self.v_th = 1.0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.membrane = torch.zeros(input_shape)\n",
    "    def __call__(self, x: 'np.ndarray[np.float32]'):\n",
    "        '''Basic implementation of IF layer.\n",
    "        Assumptions:\n",
    "        spike_threshold = 1.0,\n",
    "        spike_fn = SingleSpike\n",
    "        No batch, No timing: the FPGA simply runs everything on FIFO basis\n",
    "        Thererofe, this has to be a class.\n",
    "        x dimensions: [c, y, x]\n",
    "        '''\n",
    "        output = torch.zeros(self.membrane.shape)\n",
    "        for c in range(x.shape[0]):\n",
    "            for (i, j) in self.coordinate_pairs:\n",
    "                self.membrane[c, i, j] +=  x[c, i, j]\n",
    "\n",
    "                if self.membrane[c, i, j] > self.v_th:\n",
    "                    output[c,i,j] = 1;\n",
    "                    self.membrane[c, i, j] -= 1\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac4dea49-520a-40c5-9f96-853d3d4f1708",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class if1_layer:\n",
    "    \"\"\"Expect input shape: [time, channel, y, x]\"\"\"\n",
    "    def __init__(self, input_shape, rec=None):\n",
    "        self.timesteps = input_shape[0]\n",
    "        self.elems = torch.tensor(input_shape[1:]).prod().item()\n",
    "        self.membrane = torch.zeros([self.elems])\n",
    "        if rec != None:\n",
    "            self.recs = list()\n",
    "            self.reci = torch.tensor(rec).prod().item()\n",
    "        else:\n",
    "            self.recs = None\n",
    "        self.v_th = 1.0\n",
    "\n",
    "    def reset(self):\n",
    "        self.membrane = torch.zeros(input_shape)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        input_shape = x.shape\n",
    "        x = x.reshape((self.timesteps, self.elems))\n",
    "        output = torch.zeros((self.timesteps, self.elems))\n",
    "        recs = list()\n",
    "        for t in range(self.timesteps):\n",
    "            for e in range(self.elems):\n",
    "                self.membrane[e] +=  x[t, e]\n",
    "                if self.membrane[e] >= self.v_th:\n",
    "                    output[t, e] = 1;\n",
    "                    self.membrane[e] -= 1\n",
    "\n",
    "                if self.membrane[e] < -1:\n",
    "                    self.membrane[e] = -1\n",
    "            if self.recs != None:\n",
    "                self.recs.append(self.membrane[self.reci].item())\n",
    "\n",
    "        output = output.reshape(input_shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c559f7c9-22e7-4942-90c0-271c367cd88c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 8, 34, 34]) torch.Size([1, 100, 8, 34, 34])\n"
     ]
    }
   ],
   "source": [
    "data_me = torch.ones((100, 8, 34, 34)) * 0.2\n",
    "data_them = data_me.unsqueeze(0)\n",
    "print(data_me.shape, data_them.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9755aa0-28fd-4865-adb0-8f06de2fd8dd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def avg_pool2d(x):\n",
    "    kernel = (2,2)\n",
    "    stride = 2\n",
    "    padding = 0\n",
    "\n",
    "    # Create output buffer\n",
    "    H_out = math.floor((x.shape[1] + (2 * padding) - kernel[0]) / stride) + 1\n",
    "    W_out = math.floor((x.shape[2] + (2 * padding) - kernel[0]) / stride) + 1\n",
    "    output = torch.zeros((x.shape[0], H_out, W_out))\n",
    "\n",
    "    # Zero pad\n",
    "    data_padded = torch.zeros(x.shape[0],\n",
    "                              x.shape[1] + 2*padding,\n",
    "                              x.shape[2] + 2*padding)\n",
    "    data_padded[:, padding:x.shape[1] + padding, padding:x.shape[2] + padding] = x.cpu()\n",
    "\n",
    "    coordinate_pairs = [(a, b) for a in range(H_out) for b in range(W_out)]\n",
    "    coordinate_pairs.sort()\n",
    "    kernel_pairs = [(a, b) for a in range(kernel[0]) for b in range(kernel[1])]\n",
    "    kernel_pairs.sort()\n",
    "    for c in range(output.shape[0]):\n",
    "        for (h, w) in coordinate_pairs:\n",
    "            # Apply kernel\n",
    "            kernel_sum = 0\n",
    "            for (m, n) in kernel_pairs:\n",
    "                kernel_sum += data_padded[c, stride * h + m, stride * w + n]\n",
    "            output[c, h, w] = 1 / (kernel[0] * kernel[1]) * kernel_sum\n",
    "\n",
    "    return output                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bd8014a-d5f5-4ee7-af91-5eee35ab59d3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_module_name(input_string):\n",
    "    input_string = str(input_string)\n",
    "    # Pattern to match the function name and contents within first\n",
    "    pattern = r\"^([a-zA-Z_][a-zA-Z0-9_]*)\\(.*\"\n",
    "    \n",
    "    match = re.match(pattern, input_string)\n",
    "    if match:\n",
    "        return match.group(1)  # Return the function name\n",
    "    else:\n",
    "        return None  # If there's no match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dcf2a2-b523-483f-8d52-7de76dfeca69",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Weight Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97fe058b-dcee-45ad-a642-1ecfacc39ea5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved weights ./weights/0-Conv2d.npy : torch.Size([8, 2, 3, 3])\n",
      "Saved weights ./weights/3-Conv2d.npy : torch.Size([16, 8, 3, 3])\n",
      "Saved weights ./weights/6-Conv2d.npy : torch.Size([16, 16, 3, 3])\n",
      "Saved weights ./weights/9-Linear.npy : torch.Size([10, 256])\n",
      "\n",
      "[2024-09-26T14:52:23.508630] Saved weights\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import datetime \n",
    "snn = from_model(model=model, input_shape=(2, 34, 34), batch_size=1, spike_fn=spikegen.SingleSpike).spiking_model.to('cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (i, m) in enumerate(list(snn.modules())[1:]):\n",
    "        mod_name = get_module_name(m)\n",
    "\n",
    "        if mod_name == 'Conv2d' or mod_name == 'Linear':\n",
    "            save_name = f'./weights/{i}-{mod_name}.npy'\n",
    "            np.save(save_name, list(m.parameters())[0])\n",
    "            print(f'Saved weights {save_name} : {list(m.parameters())[0].shape}')\n",
    "\n",
    "        if mod_name == 'Flatten':\n",
    "            m = nn.Flatten(start_dim=0)\n",
    "\n",
    "print(f'\\n[{datetime.datetime.now().isoformat()}] Saved weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b50a6427-cf04-40fc-a744-85210b63d611",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './6174.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m mems \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# data = torch.tensor(np.load(f\"tensors/datafile.npy\"))\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./6174.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m outs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[0;32m~/ml-venv/lib/python3.12/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './6174.npy'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "snn = from_model(model=model, input_shape=(2, 34, 34), batch_size=1, spike_fn=spikegen.SingleSpike).spiking_model.to('cpu')\n",
    "mods = list(snn.modules())[1:]\n",
    "\n",
    "mems = list()\n",
    "\n",
    "# data = torch.tensor(np.load(f\"tensors/datafile.npy\"))\n",
    "data = torch.tensor(np.load(f\"./6174.npy\")).to(dtype=torch.float, device='cpu')\n",
    "print(f'Data Shape: {data.shape}\\n')\n",
    "outs = torch.zeros((data.shape[0], 10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    end_of_data = False\n",
    "    count = 0\n",
    "    failed = False\n",
    "    while not failed:\n",
    "        if not os.path.isfile(f\"tensors/x{count}.npy\"):\n",
    "            end_of_data = True\n",
    "            break\n",
    "\n",
    "        mod = mods[count % len(mods)]\n",
    "        mod_name = get_module_name(mod)\n",
    "\n",
    "        if count % len(mods) == 0:\n",
    "            #print(f'\\n====== BEGIN ITERATION {count // len(mods)} ======')\n",
    "            batch = count // len(mods)\n",
    "            x = data[batch].unsqueeze(0)\n",
    "            #print(f'Input sum: {x.sum()}')\n",
    "\n",
    "        # if (mod_name == 'IAFSqueeze'):\n",
    "        #     mod.record_states = True\n",
    "\n",
    "        # Get the data\n",
    "        #print(f\"Doing {mod_name}\")\n",
    "        x_hat = torch.tensor(np.load(f\"tensors/x{count}.npy\"))\n",
    "        x_new = mod(x)\n",
    "\n",
    "        # Handle wrong x case\n",
    "        x_correct = torch.isclose(x_new, x_hat, atol=1e-5)\n",
    "        correctness = torch.unique(x_correct)\n",
    "        if len(correctness) != 1 and correctness[0] == True:\n",
    "            failed = True\n",
    "            print(f'Failed at {mod_name}[{count % len(mods)}]')\n",
    "            wrong_idx = torch.stack(torch.where(x_correct == False), dim=1)\n",
    "            print(f'Total wrong {wrong_idx.shape[0]}')\n",
    "            for i in range(3):\n",
    "                r = torch.randint(0, wrong_idx.shape[0], (1,))\n",
    "                row = tuple(*(wrong_idx[r].tolist()))\n",
    "                print(f'Example {row}: {x_new[row]:.5f} != {x_hat.unsqueeze(0)[row]:.5f}')\n",
    "\n",
    "\n",
    "        if mod_name == 'Conv2d' and failed:\n",
    "            wrong_idx = torch.stack(torch.where(x_correct == False), dim=1)\n",
    "            # Check if error is on padding\n",
    "            non_boundary = False\n",
    "            for k in range(wrong_idx.shape[0]):\n",
    "                row = wrong_idx[k]\n",
    "                zeros = row[2] == 0 or row[3] == 0\n",
    "                limits = row[2] == x_new.shape[-2]-1 or row[3] == x_new.shape[-1]-1\n",
    "                if not (zeros or limits):\n",
    "                    print(f'Found non-boundary error: {row}')\n",
    "                    non_boundary = True\n",
    "\n",
    "            if not non_boundary:\n",
    "                print(f'convolution: all errors reside on boundary. Consider checking padding implementation.')\n",
    "            \n",
    "        \n",
    "        # if mod_name == 'IAFSqueeze':\n",
    "        #     recordings = mod.recordings['v_mem']\n",
    "        #     recordings_hat = torch.tensor(np.load(f\"tensors/membrane{count}.npy\")).reshape(recordings.shape)\n",
    "\n",
    "        #     mem_correct = torch.isclose(recordings_hat, recordings, atol=1e-5)\n",
    "        #     # print(f'IAF[{count % len(mods)}] Unique non-zero recordings: {len(torch.unique(recordings))-1}')\n",
    "        #     # print(f'IAF[{count % len(mods)}] Firings: {int(x_new.sum())}')\n",
    "        #     if len(torch.unique(mem_correct)) != 1 and torch.unique(mem_correct)[0] == True:\n",
    "        #         failed = True\n",
    "        #         print(f'Failed at {mod_name} - Membrane mismatch')\n",
    "        #         wrong_idx = torch.where(mem_correct == False)\n",
    "        #         wrong_i = 1\n",
    "        #         ex_pos = (int(wrong_idx[0][wrong_i]), int(wrong_idx[1][wrong_i]),\n",
    "        #                   int(wrong_idx[2][wrong_i]), int(wrong_idx[3][wrong_i]),\n",
    "        #                   int(wrong_idx[4][wrong_i]))\n",
    "        #         print(f'Example: {ex_pos}: {recordings[ex_pos]} != {recordings_hat[ex_pos[2:]]}')\n",
    "\n",
    "            \n",
    "        if (count+1) % len(mods) == 0:\n",
    "            outs[batch] = x_new[0]\n",
    "        x = x_new\n",
    "        count += 1\n",
    "    if failed:\n",
    "        print(f'\\nStopped: count {count}')\n",
    "    else:\n",
    "        print(f'\\nTest completed successfully.')\n",
    "        prediction = outs.sum(dim=0).argmax(dim=0, keepdim=True)\n",
    "        print(f'Prediction: {prediction.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23e924c8-e61f-4de8-9d01-8b9715b70a42",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datai = 6174\n",
    "for i, (realdata, label) in enumerate(snn_test_dataloader):\n",
    "    if i == datai:\n",
    "        realdata = realdata.squeeze(0)\n",
    "        np.save(f\"./tensors/torch-datafile.npy\", realdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a092410-35d3-411a-b386-5fc8cb9757ee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './6174.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Grab the data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#data = torch.tensor(np.load(f\"tensors/{datai}.npy\"))\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatai\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Run through real network\u001b[39;00m\n",
      "File \u001b[0;32m~/ml-venv/lib/python3.12/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './6174.npy'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# Grab the data\n",
    "#data = torch.tensor(np.load(f\"tensors/{datai}.npy\"))\n",
    "data = torch.tensor(np.load(f\"./{datai}.npy\")).to(dtype=torch.float, device='cpu')\n",
    "print(f'Data Shape: {data.shape}\\n')\n",
    "\n",
    "# Run through real network\n",
    "snn = from_model(model=model, input_shape=(2, 34, 34), batch_size=1, spike_fn=spikegen.SingleSpike).spiking_model.to('cpu')\n",
    "snn.eval()\n",
    "with torch.no_grad():\n",
    "    mods = list(snn.modules())[1:]\n",
    "    output = data\n",
    "    for m in mods:\n",
    "        output = m(output)\n",
    "        print(m)\n",
    "        print(output.shape)\n",
    "    lol()\n",
    "    prediction = output.sum(dim=0).argmax(dim=0, keepdim=True)\n",
    "    print(f'Prediction: {prediction.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9d98704-9d4b-4c4e-b4a3-20d38f53feda",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m realdata \u001b[38;5;241m=\u001b[39m realdata\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, realdata)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(realdata\u001b[38;5;241m.\u001b[39mshape, \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m wrong_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(torch\u001b[38;5;241m.\u001b[39mwhere(realdata \u001b[38;5;241m!=\u001b[39m data), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "for i, (realdata, label) in enumerate(snn_test_dataloader):\n",
    "    if i == datai:\n",
    "        realdata = realdata.squeeze(0)\n",
    "        np.save(f\"./{i}.npy\", realdata)\n",
    "        print(realdata.shape, data.shape)\n",
    "        print(f\"label: {label.item()}\")\n",
    "\n",
    "        wrong_idx = torch.stack(torch.where(realdata != data), dim=1)\n",
    "        print(f'Total wrong {wrong_idx.shape[0]}')\n",
    "        torch.manual_seed(0)\n",
    "        if wrong_idx.shape[0] > 0:\n",
    "            for i in range(3):\n",
    "                r = torch.randint(0, wrong_idx.shape[0], (1,))\n",
    "                row = tuple(*(wrong_idx[r].tolist()))\n",
    "                print(f'Example {row}: {data[row]:.5f} != {realdata[row]:.5f}')\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3e97817-56f9-46dc-af52-064aed68d3e9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Testing model.:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Testing model.:   0%|          | 17/10000 [00:01<09:55, 16.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Testing model.:   1%|          | 111/10000 [00:02<02:40, 61.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Testing model.:   2%|▏         | 224/10000 [00:03<01:56, 84.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Testing model.:   4%|▎         | 373/10000 [00:04<01:40, 96.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Testing model.:   5%|▍         | 468/10000 [00:06<02:12, 72.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Testing model.:   5%|▌         | 546/10000 [00:08<02:32, 61.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Testing model.:   6%|▌         | 613/10000 [00:09<02:36, 59.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Testing model.:   7%|▋         | 726/10000 [00:10<02:08, 72.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Testing model.:   8%|▊         | 804/10000 [00:13<02:58, 51.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Testing model.:   6%|▌         | 575/10000 [00:13<03:37, 43.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m output \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m---> 24\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\u001b[39;00m\n\u001b[1;32m     26\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mreshape(batch_size, n_time_steps, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/ml-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml-venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ml-venv/lib/python3.12/site-packages/sinabs/layers/iaf.py:196\u001b[0m, in \u001b[0;36mIAFSqueeze.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_data: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    194\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward call wrapper that will flatten the input to and unflatten the output from the\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    super class forward call.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml-venv/lib/python3.12/site-packages/sinabs/layers/reshape.py:83\u001b[0m, in \u001b[0;36mSqueezeMixin.squeeze_forward\u001b[0;34m(self, input_data, forward_method)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msqueeze_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_data: torch\u001b[38;5;241m.\u001b[39mTensor, forward_method: Callable):\n\u001b[1;32m     80\u001b[0m     inflated_input \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, \u001b[38;5;241m*\u001b[39minput_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     82\u001b[0m     )\n\u001b[0;32m---> 83\u001b[0m     inflated_output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43minflated_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inflated_output\u001b[38;5;241m.\u001b[39mflatten(start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, end_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/ml-venv/lib/python3.12/site-packages/sinabs/layers/lif.py:183\u001b[0m, in \u001b[0;36mLIF.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    180\u001b[0m alpha_mem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_mem_calculated\n\u001b[1;32m    181\u001b[0m alpha_syn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_syn_calculated\n\u001b[0;32m--> 183\u001b[0m spikes, state, recordings \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlif_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_mem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_mem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_syn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_syn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_buffers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspike_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspike_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspike_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspike_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43msurrogate_grad_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurrogate_grad_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_v_mem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_v_mem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecord_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_mem \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv_mem\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_syn \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi_syn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m alpha_syn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ml-venv/lib/python3.12/site-packages/sinabs/layers/functional/lif.py:73\u001b[0m, in \u001b[0;36mlif_forward\u001b[0;34m(input_data, alpha_mem, alpha_syn, state, spike_threshold, spike_fn, reset_fn, surrogate_grad_fn, min_v_mem, norm_input, record_states)\u001b[0m\n\u001b[1;32m     70\u001b[0m     recordings \u001b[38;5;241m=\u001b[39m {name: [] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m state_names}\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_time_steps):\n\u001b[0;32m---> 73\u001b[0m     spikes, state \u001b[38;5;241m=\u001b[39m \u001b[43mlif_forward_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha_mem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_mem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha_syn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha_syn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspike_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspike_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspike_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspike_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43msurrogate_grad_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msurrogate_grad_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_v_mem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_v_mem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     output_spikes\u001b[38;5;241m.\u001b[39mappend(spikes)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m record_states:\n",
      "File \u001b[0;32m~/ml-venv/lib/python3.12/site-packages/sinabs/layers/functional/lif.py:46\u001b[0m, in \u001b[0;36mlif_forward_single\u001b[0;34m(input_data, alpha_mem, alpha_syn, state, spike_threshold, spike_fn, reset_fn, surrogate_grad_fn, min_v_mem, norm_input)\u001b[0m\n\u001b[1;32m     42\u001b[0m     state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_v_mem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv_mem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 46\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mv_mem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmin_v_mem\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m min_v_mem\n\u001b[1;32m     47\u001b[0m     )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m spikes, state\n",
      "File \u001b[0;32m~/ml-venv/lib/python3.12/site-packages/torch/nn/functional.py:1489\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1477\u001b[0m threshold \u001b[38;5;241m=\u001b[39m _threshold\n\u001b[1;32m   1479\u001b[0m threshold_ \u001b[38;5;241m=\u001b[39m _add_docstr(\n\u001b[1;32m   1480\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mthreshold_,\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m,\n\u001b[1;32m   1486\u001b[0m )\n\u001b[0;32m-> 1489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrelu\u001b[39m(\u001b[38;5;28minput\u001b[39m: Tensor, inplace: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:  \u001b[38;5;66;03m# noqa: D400,D402\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"relu(input, inplace=False) -> Tensor\u001b[39;00m\n\u001b[1;32m   1491\u001b[0m \n\u001b[1;32m   1492\u001b[0m \u001b[38;5;124;03m    Applies the rectified linear unit function element-wise. See\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;124;03m    :class:`~torch.nn.ReLU` for more details.\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reset the snn for testing\n",
    "for m in mods:\n",
    "    mod_name = get_module_name(m)\n",
    "    if mod_name == \"IAFSqueeze\":\n",
    "        m.reset_states()\n",
    "        \n",
    "n_time_steps = 100\n",
    "to_raster = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=n_time_steps)\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_raster)\n",
    "snn_test_dataloader = DataLoader(snn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=False)\n",
    "snn = from_model(model=model, input_shape=(2, 34, 34), batch_size=1, spike_fn=spikegen.SingleSpike).spiking_model.to('cpu')\n",
    "\n",
    "correct_predictions = []\n",
    "with torch.no_grad():\n",
    "    test_p_bar = tqdm(snn_test_dataloader,  desc=f'Testing model.', mininterval=1)\n",
    "    for data, label in test_p_bar:\n",
    "\n",
    "        # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "        data = data.reshape(-1, 2, 34, 34).to(dtype=torch.float, device='cpu')\n",
    "        # forward\n",
    "        modules = list(snn.modules())[1:]\n",
    "        output = data\n",
    "        for m in modules:\n",
    "            output = m(output)\n",
    "        # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "        output = output.reshape(batch_size, n_time_steps, -1)\n",
    "        # accumulate all time-steps output for final prediction\n",
    "        output = output.sum(dim=1)\n",
    "        # calculate accuracy\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        # compute the total correct predictions\n",
    "        correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "        # set progressing bar\n",
    "        test_p_bar.update()\n",
    "        #test_p_bar.set_description(f\"Testing SNN Model...\")\n",
    "\n",
    "    correct_predictions = torch.cat(correct_predictions)\n",
    "    print(f\"\\nAccuracy of converted SNN: {correct_predictions.sum().item()/(len(correct_predictions))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f786645f-7a72-40ff-ba6e-26f4ee46f3c6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snn = from_model(model=model, input_shape=(2, 34, 34), batch_size=1, spike_fn=spikegen.SingleSpike).spiking_model.to('cpu')\n",
    "modules = list(snn.modules())[1:]\n",
    "iaf = modules[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3e9d250-ffbd-48f4-a35b-718f80315b1a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sinabs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m iaf \u001b[38;5;241m=\u001b[39m \u001b[43msinabs\u001b[49m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mIAFSqueeze(spike_fn\u001b[38;5;241m=\u001b[39mspikegen\u001b[38;5;241m.\u001b[39mSingleSpike, min_v_mem\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, record_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sinabs' is not defined"
     ]
    }
   ],
   "source": [
    "iaf = sinabs.layers.IAFSqueeze(spike_fn=spikegen.SingleSpike, min_v_mem=-1, batch_size=1, record_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "141dceb7-1444-43cf-8492-ef66c5b2ad52",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape = [100, 8, 34, 34]\n",
    "rec = (5,0,24)\n",
    "data = torch.ones(shape) * 0.2\n",
    "\n",
    "my_if = if1_layer(shape, rec)\n",
    "\n",
    "out = iaf(data.clone())\n",
    "out_hat = my_if(data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ddb30fe4-ac7c-415b-8795-b0188a6c8cdd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wrong 0\n"
     ]
    }
   ],
   "source": [
    "wrong_idx = torch.stack(torch.where(out_hat != out), dim=1)\n",
    "print(f'Total wrong {wrong_idx.shape[0]}')\n",
    "torch.manual_seed(0)\n",
    "if wrong_idx.shape[0] > 0:\n",
    "    for i in range(3):\n",
    "        r = torch.randint(0, wrong_idx.shape[0], (1,))\n",
    "        row = tuple(*(wrong_idx[r].tolist()))\n",
    "        print(f'Example {row}: {out[row]:.5f} != {out_hat[row]:.5f}')\n",
    "\n",
    "    c = 3\n",
    "    x = 0\n",
    "    y = 24\n",
    "    for i in range(shape[0]):\n",
    "        s = (i, c, x,y)\n",
    "        print(f\"{s}: {out[s]:5} ({iaf.recordings['v_mem'][(0,i) + rec]:5.5f}) | {out_hat[s]:5} ({my_if.recs[i]:5.5f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89485c25-8e4c-4902-8936-be34f718d0d9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f: tensor([True])\n",
      "pred: 2\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(np.load(\"./tensors/data.npy\"))\n",
    "snn = from_model(model=model, input_shape=(2, 34, 34), batch_size=1, spike_fn=spikegen.SingleSpike).spiking_model.to('cpu')\n",
    "modules = list(snn.modules())[1:]\n",
    "c0 = modules[0]\n",
    "i1 = modules[1]\n",
    "a2 = modules[2]\n",
    "c3 = modules[3]\n",
    "i4 = modules[4]\n",
    "a5 = modules[5]\n",
    "c6 = modules[6]\n",
    "i7 = modules[7]\n",
    "f8 = modules[8]\n",
    "l9 = modules[9]\n",
    "i10 = modules[10]\n",
    "\n",
    "x1 = c0(data)\n",
    "x1 = i1(x1)\n",
    "x1 = a2(x1)\n",
    "x1 = c3(x1)\n",
    "x1 = i4(x1)\n",
    "x1 = a5(x1)\n",
    "x1 = c6(x1)\n",
    "x1 = i7(x1)\n",
    "x1 = f8(x1)\n",
    "x1 = l9(x1)\n",
    "x1 = i10(x1)\n",
    "x2 = torch.tensor(np.load(\"./tensors/out.npy\"))\n",
    "blah = False\n",
    "if blah:\n",
    "    print(f\"e: {torch.unique(x1 == x2)}\")\n",
    "else:\n",
    "    print(f\"f: {torch.unique(torch.isclose(x1, x2, atol=1e-5))}\")\n",
    "\n",
    "x1 = x1.sum(dim=0)\n",
    "x1 = x1.argmax()\n",
    "print(f\"pred: {x1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bf36fbb-5b8f-4b22-95ff-db6609d64b37",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 3.68% occupancy.\n",
      "1: 3.73% occupancy.\n",
      "2: 1.49% occupancy.\n",
      "3: 3.78% occupancy.\n"
     ]
    }
   ],
   "source": [
    "snn = from_model(model=model, input_shape=(2, 34, 34), batch_size=1, spike_fn=spikegen.SingleSpike).spiking_model.to('cpu')\n",
    "modules = list(snn.modules())[1:]\n",
    "c0 = modules[0]\n",
    "i1 = modules[1]\n",
    "a2 = modules[2]\n",
    "c3 = modules[3]\n",
    "i4 = modules[4]\n",
    "a5 = modules[5]\n",
    "c6 = modules[6]\n",
    "i7 = modules[7]\n",
    "f8 = modules[8]\n",
    "l9 = modules[9]\n",
    "i10 = modules[10]\n",
    "\n",
    "stop = 100\n",
    "spike_percentages = [0.0, 0.0, 0.0, 0.0]\n",
    "for datai, (data, _) in enumerate(snn_test_dataloader):\n",
    "    data = data.reshape([100, 2, 34, 34]).to(dtype=torch.float)\n",
    "    x = a2(i1(c0(data))) # Let's do after avgpool because that's where it matters for our hw\n",
    "    spike_percentages[0] += x.nonzero().shape[0] / x.numel()\n",
    "    x = a5(i4(c3(x)))\n",
    "    spike_percentages[1] += x.nonzero().shape[0] / x.numel()\n",
    "    x = i7(c6(x))\n",
    "    spike_percentages[2] += x.nonzero().shape[0] / x.numel()\n",
    "    x = i10(l9(f8(x)))\n",
    "    spike_percentages[3] += x.nonzero().shape[0] / x.numel()\n",
    "    if datai > stop:\n",
    "        break\n",
    "\n",
    "for layer, perc in enumerate(spike_percentages):\n",
    "    print(f\"{layer}: {(perc / stop) * 100:.2f}% occupancy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "188eb9b0-b3cd-49bc-9ef1-267d1f8c7ff9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9248\n",
      "4624\n",
      "256\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14138"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(i1(c0(data))[0].numel())\n",
    "print(i4(c3(a2(i1(c0(data)))))[0].numel())\n",
    "print(i7(c6(a5(i4(c3(a2(i1(c0(data))))))))[0].numel())\n",
    "print(i10(l9(f8(i7(c6(a5(i4(c3(a2(i1(c0(data)))))))))))[0].numel())\n",
    "9248 + 4624 + 256 + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb1faa2-7834-4486-9971-ff5198e43b21",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m      5\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      6\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(frames[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "frames = data[0].squeeze()\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].imshow(frames[0])\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(frames[1])\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/mrontio/uni/phd/documents/feasibility-study/img/scnn-input.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "29eb9654-050b-4be4-bda1-da80755c91f6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462.4 KB\n"
     ]
    }
   ],
   "source": [
    "print(f\"{(np.prod(list(data.shape)) * 16) / 8 / 1000} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "897aa527-0ea2-44e7-b5fb-ecd0c5623d73",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "924.8 KB\n"
     ]
    }
   ],
   "source": [
    "buffers = [[100, 8, 34, 34], [100, 16, 17, 17], [100, 16, 4, 4], [100, 10]] # if we buffer at end of IF uni\n",
    "buffers = [[100, 8, 17, 17], [100, 16, 8, 8], [100, 16, 4, 4], [100, 10]] # if we buffer at end of pool unit\n",
    "total = 0\n",
    "for b in buffers:\n",
    "    total += np.prod(b)\n",
    "print(f\"{(np.prod(list(data.shape)) * 32) / 8 / 1000} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4d702d5f-4472-41f4-8031-ff871eed403c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n"
     ]
    }
   ],
   "source": [
    "def count_neurons(model):\n",
    "    total_neurons = 0\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            total_neurons += layer.out_features  # Each output unit is a neuron\n",
    "        elif isinstance(layer, nn.Conv2d):\n",
    "            total_neurons += layer.out_channels * layer.kernel_size[0] * layer.kernel_size[1]  # Neurons in convolution\n",
    "    return total_neurons\n",
    "\n",
    "print(count_neurons(mob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "id": "38242ec6-997d-4814-bb77-c51803ecec76",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wrong 10185\n",
      "Example (81, 2, 3, 2): 0.08786 != 0.00000\n",
      "Example (10, 14, 1, 2): 0.22917 != 0.00000\n",
      "Example (25, 10, 3, 3): -0.01374 != 0.00000\n"
     ]
    }
   ],
   "source": [
    "wrong_idx = torch.stack(torch.where(x1 != x2), dim=1)\n",
    "print(f'Total wrong {wrong_idx.shape[0]}')\n",
    "#torch.manual_seed(0)\n",
    "if wrong_idx.shape[0] > 0:\n",
    "    for i in range(3):\n",
    "        r = torch.randint(0, wrong_idx.shape[0], (1,))\n",
    "        row = tuple(*(wrong_idx[r].tolist()))\n",
    "        print(f'Example {row}: {x1[row]:.5f} != {x2[row]:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "64e9b2a7-828a-4796-b55c-9964452dbc50",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = x1.sum(dim=0)\n",
    "x1.argmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82139bd2-9579-4ec2-aa31-aa1f290daca9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getIndex(source, dims):\n",
    "    v_index = dims[0]\n",
    "    for i in range(1, len(source)):\n",
    "        v_index = v_index * source[i] + dims[i]\n",
    "    return v_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8ed8b7b7-eff1-4302-835c-2e7028098ae2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178110\n",
      "712 KB\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for k in model_weights.keys():\n",
    "    total += np.prod(model_weights[k].shape)\n",
    "print(total)    \n",
    "print(f\"{(total * 32) // 8 // 1000} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "29c045d9-70a5-46e0-bcfb-3992eafe18de",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['0.weight', '3.weight', '6.weight', '9.weight'])\n"
     ]
    }
   ],
   "source": [
    "print(model_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a819194f-cdb1-4c46-84b0-b79d8cf8293a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "val = 3.5\n",
    "with open(\"./weights.coe\", \"w\") as f:\n",
    "    f.write(\"; Weight BRAM initialization,\\n; Length = 6160\\n\")\n",
    "    f.write(\"memory_initialization_radix=16;\\n\")\n",
    "    f.write(\"memory_initialization_vector=\\n\")\n",
    "\n",
    "    for i in range(32):\n",
    "        h = hex(struct.unpack('I', struct.pack('f', val))[0])[2:]\n",
    "        f.write(f\"{h},\\n\")\n",
    "\n",
    "    f.seek(f.tell() - 2)\n",
    "    f.write(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1c811b4b-830f-431f-af56-abec8153f8b9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (100, 2, 34, 34), Label: 1\n"
     ]
    }
   ],
   "source": [
    "sample, label = snn_test_dataset[1000]\n",
    "sample = sample.squeeze()\n",
    "print(f'Shape: {sample.shape}, Label: {label}')\n",
    "\n",
    "with open(\"/home/mrontio/pynq-share/jupyter_notebooks/data/nmnist-converted/sample.npy\", \"wb\") as f:\n",
    "    np.save(f, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9178b584-b5a7-40b6-b39c-992ff8ce912e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int timeframe_size = 2312;\n",
      "int timeframe[timeframe_size] = { \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
      "\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n"
     ]
    }
   ],
   "source": [
    "c_array = \"\"\n",
    "for i, val in enumerate(timeframe):\n",
    "    if (i % 20) == 0:\n",
    "        c_array += \"\\n\\t\\t\"\n",
    "    c_array += f\"{val}, \"\n",
    "    \n",
    "c_array = c_array.rstrip(\", \")\n",
    "print(f\"int timeframe_size = {timeframe.size};\\nu8 timeframe[] = {{ {c_array}}};\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e68f72f2-0f69-4b1b-b965-a253ac2d7afb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'please yes'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "what = \"please yes, \"\n",
    "what.rstrip(', ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "name": "snn-training.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
